{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "cudnn.benchmark = True  # fire on all cylinders\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import sys\n",
    "from functools import partial\n",
    "\n",
    "sys.path.insert(0, 'wrn.py')\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkDatasetDetection(torch.utils.data.Dataset):\n",
    "    def __init__(self, model_folder):\n",
    "        super().__init__()\n",
    "        model_paths = []\n",
    "\n",
    "        for x in sorted(os.listdir(os.path.join(model_folder, 'clean'))):\n",
    "          if not x.startswith('.') and not x.endswith('(1)'):\n",
    "            model_paths.append(os.path.join(model_folder, 'clean', x))\n",
    "\n",
    "        for x in sorted(os.listdir(os.path.join(model_folder, 'trojan'))):\n",
    "          if not x.startswith('.'):\n",
    "            model_paths.append(os.path.join(model_folder, 'trojan', x))\n",
    "            \n",
    "        labels = []\n",
    "        data_sources = []\n",
    "        for p in model_paths:\n",
    "            with open(os.path.join(p, 'info.json'), 'r') as f:\n",
    "                info = json.load(f)\n",
    "                data_sources.append(info['dataset'])\n",
    "            if p.split('/')[-2] == 'clean':\n",
    "                labels.append(0)\n",
    "            elif p.split('/')[-2] == 'trojan':\n",
    "                labels.append(1)\n",
    "            else:\n",
    "                raise ValueError('unexpected path {}'.format(p))\n",
    "        self.model_paths = model_paths\n",
    "        self.labels = labels\n",
    "        self.data_sources = data_sources\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.model_paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return torch.load(os.path.join(self.model_paths[index], 'model.pt')), \\\n",
    "               self.labels[index], self.data_sources[index]\n",
    "\n",
    "def custom_collate(batch):\n",
    "    return [x[0] for x in batch], [x[1] for x in batch], [x[2] for x in batch]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "Spliting off a validation set from the train set for testing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '../../tdc_datasets'\n",
    "task = 'detection'\n",
    "dataset = NetworkDatasetDetection(os.path.join(dataset_path, task, 'train'))\n",
    "\n",
    "split = int(len(dataset) * 0.8)\n",
    "rnd_idx = np.random.permutation(len(dataset))\n",
    "train_dataset = torch.utils.data.Subset(dataset, rnd_idx[:split])\n",
    "val_dataset = torch.utils.data.Subset(dataset, rnd_idx[split:])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=10, shuffle=True,\n",
    "                                           num_workers=0, pin_memory=False, collate_fn=custom_collate)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=10,\n",
    "                                           num_workers=0, pin_memory=False, collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct the MNTD network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sources = ['CIFAR-10', 'CIFAR-100', 'GTSRB', 'MNIST']\n",
    "data_source_to_channel = {k: 1 if k == 'MNIST' else 3 for k in data_sources}\n",
    "data_source_to_resolution = {k: 28 if k == 'MNIST' else 32 for k in data_sources}\n",
    "data_source_to_depth = {k: 3 if k == 'GTSRB' else 1 for k in data_sources}\n",
    "data_source_to_hidden_resolution = {'CIFAR-10': 64*128, 'CIFAR-100': 64*128, 'GTSRB': 128, 'MNIST': 128}\n",
    "data_source_to_num_classes = {'CIFAR-10': 10, 'CIFAR-100': 100, 'GTSRB': 43, 'MNIST': 10}\n",
    "\n",
    "class MetaNetwork(nn.Module):\n",
    "    def __init__(self, num_queries, num_classes=1):\n",
    "        super().__init__()\n",
    "        self.queries = nn.ParameterDict(\n",
    "            {k: nn.Parameter(torch.rand(num_queries,\n",
    "                                        data_source_to_channel[k],\n",
    "                                        data_source_to_resolution[k],\n",
    "                                        data_source_to_resolution[k])) for k in data_sources}\n",
    "        )\n",
    "        #Method 1: Extract hidden layer of model and add convolutional layer for more info, reduce overfitting\n",
    "        #Method 2: just put dropouts\n",
    "        self.affines = nn.ModuleDict(\n",
    "            {\n",
    "                'CIFAR-10': nn.Linear(10*64*128, 512),\n",
    "                'CIFAR-100': nn.Linear(10*64*128, 512),\n",
    "                'GTSRB': nn.Linear(10 * 128, 512),\n",
    "                'MNIST': nn.Linear(10 * 128, 512)\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        self.layer_output = {}\n",
    "        self.norm1 = nn.LayerNorm(512)\n",
    "        self.norm2 = nn.LayerNorm(64)\n",
    "        self.dropout = nn.Dropout(0.20)\n",
    "        self.relu = nn.ReLU(True)\n",
    "        self.linear1 = nn.Linear(512, 64)\n",
    "        self.linear2 = nn.Linear(128, 32)\n",
    "        self.final_output = nn.Linear(512, num_classes)\n",
    "    \n",
    "    def get_all_layers(self, net):\n",
    "        def hook_fn(m, i, o, n=\"\"):\n",
    "            self.layer_output[n] = o\n",
    "\n",
    "        for name, layer in net.named_modules():\n",
    "            if hasattr(layer, \"_module\") and layer._module:\n",
    "                self.get_all_layers(layer)\n",
    "            elif hasattr(layer, \"_parameters\") and layer._parameters:\n",
    "                # it's a non sequential. Register a hook\n",
    "                layer.register_forward_hook(partial(hook_fn, n=name))\n",
    "\n",
    "    def get_layer(self, depth):\n",
    "        layers = []\n",
    "        \n",
    "        for k, v in self.layer_output.items():\n",
    "            layers.append(v)\n",
    "\n",
    "        self.layer_output.clear()\n",
    "        return layers[-1 * depth]\n",
    "    \n",
    "    def forward(self, net, data_source):\n",
    "        \"\"\"\n",
    "        :param net: an input network of one of the model_types specified at init\n",
    "        :param data_source: the name of the data source\n",
    "        :returns: a score for whether the network is a Trojan or not\n",
    "        \"\"\"\n",
    "        query = self.queries[data_source]\n",
    "        self.get_all_layers(net)\n",
    "        net(query)\n",
    "        out = self.get_layer(2)\n",
    "        out = out.view(1, -1)\n",
    "        out = self.affines[data_source](out)\n",
    "        out = self.norm1(out)\n",
    "        out = self.relu(out)\n",
    "        return self.final_output(out)\n",
    "\n",
    "    \n",
    "# torch.Size([10, 128, 8, 8])\n",
    "# torch.Size([10, 128, 8, 8])\n",
    "# torch.Size([10, 3, 32, 32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d54d9ccfdb7941fcb88792a149e9cafa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b5e9873fff24fa2901e5c48ffa5bd38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fd6595c1b00483a9a120e1d10968b8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b40366c70d2a4756905d88a6faf03174",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6955a8a19ec44c19eb5ece08c1ce0d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "meta_network = MetaNetwork(10, num_classes=1).cpu().train()\n",
    "\n",
    "num_epochs = 5\n",
    "lr = 0.05\n",
    "weight_decay = 0.\n",
    "optimizer = torch.optim.Adam(meta_network.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs * len(train_dataset))\n",
    "\n",
    "loss_ema = np.inf\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    pbar = tqdm(train_loader)\n",
    "    pbar.set_description(f\"Epoch {epoch + 1}\")\n",
    "    for i, (net, label, data_source) in enumerate(pbar):\n",
    "        net = net[0]\n",
    "        label = label[0]\n",
    "        data_source = data_source[0]\n",
    "        net.cpu().eval()\n",
    "        \n",
    "        out = meta_network(net, data_source)\n",
    "        \n",
    "        loss = F.binary_cross_entropy_with_logits(out, torch.FloatTensor([label]).unsqueeze(0).cpu())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(inputs=list(meta_network.parameters()))\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        for k in meta_network.queries.keys():\n",
    "            meta_network.queries[k].data = meta_network.queries[k].data.clamp(0, 1)\n",
    "        loss_ema = loss.item() if loss_ema == np.inf else 0.95 * loss_ema + 0.05 * loss.item()\n",
    "\n",
    "        pbar.set_postfix(loss=loss_ema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MetaNetwork(\n",
       "  (queries): ParameterDict(\n",
       "      (CIFAR-10): Parameter containing: [torch.FloatTensor of size 10x3x32x32]\n",
       "      (CIFAR-100): Parameter containing: [torch.FloatTensor of size 10x3x32x32]\n",
       "      (GTSRB): Parameter containing: [torch.FloatTensor of size 10x3x32x32]\n",
       "      (MNIST): Parameter containing: [torch.FloatTensor of size 10x1x28x28]\n",
       "  )\n",
       "  (affines): ModuleDict(\n",
       "    (CIFAR-10): Linear(in_features=81920, out_features=512, bias=True)\n",
       "    (CIFAR-100): Linear(in_features=81920, out_features=512, bias=True)\n",
       "    (GTSRB): Linear(in_features=1280, out_features=512, bias=True)\n",
       "    (MNIST): Linear(in_features=1280, out_features=128, bias=True)\n",
       "  )\n",
       "  (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (linear1): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (linear2): Linear(in_features=128, out_features=32, bias=True)\n",
       "  (final_output): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_network.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "\n",
    "def evaluate(meta_network, loader):\n",
    "    loss_list = []\n",
    "    correct_list = []\n",
    "    confusion_matrix = torch.zeros(2,2)\n",
    "    all_scores = []\n",
    "    all_labels = []\n",
    "    \n",
    "\n",
    "    for i, (net, label, data_source) in enumerate(tqdm(loader)):\n",
    "        net[0].cpu().eval()\n",
    "        with torch.no_grad():\n",
    "            out = meta_network(net[0], data_source[0])\n",
    "            scores.append(out)\n",
    "        loss = F.binary_cross_entropy_with_logits(out, torch.FloatTensor([label[0]]).unsqueeze(0).cpu())\n",
    "        correct = int((out.squeeze() > 0).int().item() == label[0])\n",
    "        loss_list.append(loss.item())\n",
    "        correct_list.append(correct)\n",
    "        confusion_matrix[(out.squeeze() > 0).int().item(), label[0]] += 1\n",
    "        all_scores.append(out.squeeze().item())\n",
    "        all_labels.append(label[0])\n",
    "        \n",
    "    \n",
    "    return np.mean(loss_list), np.mean(correct_list), confusion_matrix, all_labels, all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d62b5e274984400968b51b0c851429c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.739, Train Accuracy: 50.00\n",
      "Confusion Matrix:\n",
      " [[ 6.  5.]\n",
      " [35. 34.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9f/hxlqlv390w7cmv6khqywk6ch0000gn/T/ipykernel_12542/2791898451.py:4: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  np.save('predictions.npy', np.array(scores))\n",
      "/var/folders/9f/hxlqlv390w7cmv6khqywk6ch0000gn/T/ipykernel_12542/2791898451.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  np.save('predictions.npy', np.array(scores))\n"
     ]
    }
   ],
   "source": [
    "loss, acc, cmat, _, _ = evaluate(meta_network, train_loader)\n",
    "print(f'Train Loss: {loss:.3f}, Train Accuracy: {acc*100:.2f}')\n",
    "print('Confusion Matrix:\\n', cmat.numpy())\n",
    "np.save('predictions.npy', np.array(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0077]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "net = torch.load('../../tdc_datasets/detection/train/clean/id-0000/model.pt')\n",
    "print(meta_network(net, 'CIFAR-10'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b387a07d43945aaa94ca6bcb6bcc658",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.725, Val Accuracy: 50.00\n",
      "Confusion Matrix:\n",
      " [[2. 2.]\n",
      " [8. 8.]]\n"
     ]
    }
   ],
   "source": [
    "loss, acc, cmat, all_labels, all_preds = evaluate(meta_network, val_loader)\n",
    "print(f'Val Loss: {loss:.3f}, Val Accuracy: {acc*100:.2f}')\n",
    "print('Confusion Matrix:\\n', cmat.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val AUROC: 0.540\n"
     ]
    }
   ],
   "source": [
    "print(f'Val AUROC: {roc_auc_score(all_labels, all_preds):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class for the validation/test sets, which contain all networks in a single folder\n",
    "\n",
    "class NetworkDatasetDetectionTest(torch.utils.data.Dataset):\n",
    "    def __init__(self, model_folder):\n",
    "        super().__init__()\n",
    "        model_paths = [os.path.join(model_folder, x) for x in sorted(os.listdir(os.path.join(model_folder)))]\n",
    "        data_sources = []\n",
    "        for model_path in model_paths:\n",
    "            with open(os.path.join(model_path, 'info.json'), 'r') as f:\n",
    "                info = json.load(f)\n",
    "                data_sources.append(info['dataset'])\n",
    "        self.model_paths = model_paths\n",
    "        self.data_sources = data_sources\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.model_paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return torch.load(os.path.join(self.model_paths[index], 'model.pt')), self.data_sources[index]\n",
    "\n",
    "def custom_collate(batch):\n",
    "    return [x[0] for x in batch], [x[1] for x in batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '../../tdc_datasets'\n",
    "task = 'detection'\n",
    "\n",
    "test_dataset = NetworkDatasetDetectionTest(os.path.join(dataset_path, task, 'val'))\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False,\n",
    "                                          num_workers=0, pin_memory=False, collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(meta_network, loader):\n",
    "    \n",
    "    all_scores = []\n",
    "    for i, (net, data_source) in enumerate(tqdm(loader)):\n",
    "        net[0].cpu().eval()\n",
    "        with torch.no_grad():\n",
    "            out = meta_network(net[0], data_source[0])\n",
    "        all_scores.append(out.squeeze().item())\n",
    "    \n",
    "    return all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(meta_network, 'meta_network.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.save(.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae3a5d51415b439b86de5a806e4385d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores = predict(meta_network, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: predictions.npy (deflated 48%)\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('mntd_submission'):\n",
    "    os.makedirs('mntd_submission')\n",
    "\n",
    "with open(os.path.join('mntd_submission', 'predictions.npy'), 'wb') as f:\n",
    "    np.save(f, np.array(scores))\n",
    "\n",
    "!cd mntd_submission && zip ../mntd_submission.zip ./* && cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md                \u001b[34mmntd_submission\u001b[m\u001b[m          wrn.py\n",
      "\u001b[34m__pycache__\u001b[m\u001b[m              mntd_submission.zip\n",
      "example_submission.ipynb utils.py\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7998d61e83ea8a772e1e4cebc779da92897ffb5aefd0cb443f0c064e6f8234f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
